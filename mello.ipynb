{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15583338-70e3-4c70-b974-7931c5d171d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lit-gpt'...\n",
      "remote: Enumerating objects: 5300, done.\u001b[K\n",
      "remote: Counting objects: 100% (1517/1517), done.\u001b[K\n",
      "remote: Compressing objects: 100% (420/420), done.\u001b[K\n",
      "remote: Total 5300 (delta 1307), reused 1170 (delta 1094), pack-reused 3783\u001b[K\n",
      "Receiving objects: 100% (5300/5300), 1.81 MiB | 7.13 MiB/s, done.\n",
      "Resolving deltas: 100% (3626/3626), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Lightning-AI/lit-gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4add27-848d-4ced-badd-b36e22e12e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'lit-gpt'\n",
      "/workspace/lit-gpt\n"
     ]
    }
   ],
   "source": [
    "cd lit-gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd61318-457a-42e9-beee-a4bf139cfcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/lit-gpt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d4f446-0225-4bca-a869-91e32f3100d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3 (from -r requirements.txt (line 2))\n",
      "  Cloning https://github.com/Lightning-AI/lightning (to revision 6cbe9ceb560d798892bdae9186291acf9bf5d2e3) to /tmp/pip-install-rybc7ofv/lightning_5157331daff2408cbdd959ed3fae6ae3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-rybc7ofv/lightning_5157331daff2408cbdd959ed3fae6ae3\n",
      "  Running command git rev-parse -q --verify 'sha^6cbe9ceb560d798892bdae9186291acf9bf5d2e3'\n",
      "  Running command git fetch -q https://github.com/Lightning-AI/lightning 6cbe9ceb560d798892bdae9186291acf9bf5d2e3\n",
      "  Running command git checkout -q 6cbe9ceb560d798892bdae9186291acf9bf5d2e3\n",
      "  Resolved https://github.com/Lightning-AI/lightning to commit 6cbe9ceb560d798892bdae9186291acf9bf5d2e3\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/EleutherAI/lm-evaluation-harness.git@115206dc89dad67b8beaa90051fb52db77f0a529 (from -r requirements-all.txt (line 13))\n",
      "  Cloning https://github.com/EleutherAI/lm-evaluation-harness.git (to revision 115206dc89dad67b8beaa90051fb52db77f0a529) to /tmp/pip-req-build-wchg4z3m\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm-evaluation-harness.git /tmp/pip-req-build-wchg4z3m\n",
      "  Running command git rev-parse -q --verify 'sha^115206dc89dad67b8beaa90051fb52db77f0a529'\n",
      "  Running command git fetch -q https://github.com/EleutherAI/lm-evaluation-harness.git 115206dc89dad67b8beaa90051fb52db77f0a529\n",
      "  Running command git checkout -q 115206dc89dad67b8beaa90051fb52db77f0a529\n",
      "  Resolved https://github.com/EleutherAI/lm-evaluation-harness.git to commit 115206dc89dad67b8beaa90051fb52db77f0a529\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu118)\n",
      "Collecting bitsandbytes==0.41.0 (from -r requirements-all.txt (line 2))\n",
      "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting scipy (from -r requirements-all.txt (line 3))\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from -r requirements-all.txt (line 4))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers (from -r requirements-all.txt (line 5))\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting datasets (from -r requirements-all.txt (line 6))\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting zstandard (from -r requirements-all.txt (line 7))\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting pandas (from -r requirements-all.txt (line 8))\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pyarrow (from -r requirements-all.txt (line 9))\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting tensorboard (from -r requirements-all.txt (line 10))\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting torchmetrics (from -r requirements-all.txt (line 11))\n",
      "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jsonargparse[signatures] (from -r requirements.txt (line 3))\n",
      "  Downloading jsonargparse-4.27.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2)) (6.0.1)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2))\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2)) (1.24.1)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2)) (23.2)\n",
      "Collecting tqdm<6.0,>=4.57.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2))\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2))\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r requirements.txt (line 3))\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r requirements.txt (line 3))\n",
      "  Downloading typeshed_client-2.4.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers->-r requirements-all.txt (line 5))\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements-all.txt (line 6)) (2.31.0)\n",
      "Collecting xxhash (from datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements-all.txt (line 8)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements-all.txt (line 8))\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->-r requirements-all.txt (line 8))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting protobuf<4.24,>=3.19.6 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements-all.txt (line 10)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->-r requirements-all.txt (line 10)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting einops (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib-resources (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jsonlines (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numexpr (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting openai>=0.6.4 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading openai-1.3.8-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting omegaconf>=2.2 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.2.0 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pybind11>=2.6.2 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pycountry (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pycountry-23.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pytablewriter (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu==1.5.0 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.24.1 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting sqlitedict (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm-multiprocess (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Collecting transformers>=4.1 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.17.1 (from lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting portalocker (from sacrebleu==1.5.0->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting lightning-cloud (from lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading lightning_cloud-0.5.57-py3-none-any.whl.metadata (933 bytes)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.17.1->lm-eval==0.3.0->-r requirements-all.txt (line 13)) (5.9.6)\n",
      "Collecting safetensors>=0.3.1 (from accelerate>=0.17.1->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements-all.txt (line 6)) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements-all.txt (line 6))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers->-r requirements-all.txt (line 5))\n",
      "  Downloading huggingface_hub-0.19.3-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.19.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.19.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<2025.0,>2021.06.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.2->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13)) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13)) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13)) (1.3.0)\n",
      "Collecting typing-extensions (from torch>=2.1.0->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements-all.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements-all.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements-all.txt (line 6)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements-all.txt (line 6)) (2022.12.7)\n",
      "Collecting nltk (from rouge-score>=0.0.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn>=0.24.1->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.24.1->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.1->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements-all.txt (line 10)) (2.1.2)\n",
      "Collecting click (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastapi (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15)) (2.3.0)\n",
      "Collecting python-multipart (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting uvicorn (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15)) (1.6.4)\n",
      "Collecting boto3 (from lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading boto3-1.33.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading tcolorpy-0.1.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Collecting colorama (from tqdm-multiprocess->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13)) (1.1.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements-all.txt (line 10))\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements-all.txt (line 10)) (3.2.0)\n",
      "Collecting botocore<1.34.0,>=1.33.11 (from boto3->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading botocore-1.33.11-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=0.6.4->lm-eval==0.3.0->-r requirements-all.txt (line 13))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15)) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->lightning-cloud->lightning[data]@ git+https://github.com/Lightning-AI/lightning@6cbe9ceb560d798892bdae9186291acf9bf5d2e3->-r requirements-all.txt (line 15))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.2/184.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.3.8-py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeshed_client-2.4.0-py3-none-any.whl (594 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonargparse-4.27.1-py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading lightning_cloud-0.5.57-py3-none-any.whl (853 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.1/853.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.1/384.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pycountry-23.12.7-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
      "Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.33.11-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading botocore-1.33.11-py3-none-any.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lightning, lm-eval, antlr4-python3-runtime, rouge-score, sqlitedict\n",
      "  Building wheel for lightning (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightning: filename=lightning-2.2.0.dev0-py3-none-any.whl size=2019040 sha256=20af6167bdb0a59b8d2f8a36fc2bb78e41dae1f651bead15339fa533d273adff\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/bb/f5/17e1e79f07a994cfe40a1bd94bee5ea58771ac8cc3132d7607\n",
      "  Building wheel for lm-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lm-eval: filename=lm_eval-0.3.0-py3-none-any.whl size=2660363 sha256=647d9383acd28da4e57b972ad2661877ff64393eab454cec8bcf9bb2c7b38b91\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/5f/52/6150b0db7e42c8977c1a5fc7d208eb8e77dfad895424d142ef\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=30d262673499be62ae31050f9782e59fc9e02a84974ccd831893418ba94a6893\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=8ebb215dc6c9d0ec3daae6d880a2076a8cd5fb350381d56bb6a2a18d485dd325\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=d180c4977b0cf8b535fc1627abf73eec355dace8c8efe8bd2e00dd2fb958748e\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "Successfully built lightning lm-eval antlr4-python3-runtime rouge-score sqlitedict\n",
      "Installing collected packages: sqlitedict, sentencepiece, pytz, bitsandbytes, antlr4-python3-runtime, zstandard, xxhash, werkzeug, tzdata, typing-extensions, tqdm, threadpoolctl, tensorboard-data-server, tcolorpy, scipy, safetensors, regex, python-multipart, pycountry, pybind11, pyasn1, pyarrow-hotfix, pyarrow, protobuf, portalocker, pathvalidate, omegaconf, numexpr, multidict, mdurl, markdown, jsonlines, jsonargparse, joblib, jmespath, importlib-resources, h11, grpcio, fsspec, frozenlist, einops, docstring-parser, dill, colorama, click, chardet, cachetools, async-timeout, anyio, annotated-types, absl-py, yarl, uvicorn, typeshed-client, tqdm-multiprocess, starlette, scikit-learn, sacrebleu, rsa, requests-oauthlib, pydantic-core, pyasn1-modules, pandas, nltk, multiprocess, mbstrdecoder, markdown-it-py, lightning-utilities, huggingface_hub, httpcore, botocore, aiosignal, typepy, torchmetrics, tokenizers, s3transfer, rouge-score, rich, pydantic, httpx, google-auth, aiohttp, accelerate, transformers, openai, google-auth-oauthlib, fastapi, boto3, tensorboard, pytorch-lightning, peft, lightning-cloud, datasets, DataProperty, tabledata, lightning, pytablewriter, lm-eval\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed DataProperty-1.0.1 absl-py-2.0.0 accelerate-0.25.0 aiohttp-3.9.1 aiosignal-1.3.1 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 anyio-3.7.1 async-timeout-4.0.3 bitsandbytes-0.41.0 boto3-1.33.11 botocore-1.33.11 cachetools-5.3.2 chardet-5.2.0 click-8.1.7 colorama-0.4.6 datasets-2.15.0 dill-0.3.7 docstring-parser-0.15 einops-0.7.0 fastapi-0.104.1 frozenlist-1.4.0 fsspec-2023.10.0 google-auth-2.25.2 google-auth-oauthlib-1.1.0 grpcio-1.60.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface_hub-0.19.4 importlib-resources-6.1.1 jmespath-1.0.1 joblib-1.3.2 jsonargparse-4.27.1 jsonlines-4.0.0 lightning-2.2.0.dev0 lightning-cloud-0.5.57 lightning-utilities-0.10.0 lm-eval-0.3.0 markdown-3.5.1 markdown-it-py-3.0.0 mbstrdecoder-1.1.3 mdurl-0.1.2 multidict-6.0.4 multiprocess-0.70.15 nltk-3.8.1 numexpr-2.8.7 omegaconf-2.3.0 openai-1.3.8 pandas-2.1.4 pathvalidate-3.2.0 peft-0.7.0 portalocker-2.8.2 protobuf-4.23.4 pyarrow-14.0.1 pyarrow-hotfix-0.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pybind11-2.11.1 pycountry-23.12.7 pydantic-2.5.2 pydantic-core-2.14.5 pytablewriter-1.2.0 python-multipart-0.0.6 pytorch-lightning-2.1.2 pytz-2023.3.post1 regex-2023.10.3 requests-oauthlib-1.3.1 rich-13.7.0 rouge-score-0.1.2 rsa-4.9 s3transfer-0.8.2 sacrebleu-1.5.0 safetensors-0.4.1 scikit-learn-1.3.2 scipy-1.11.4 sentencepiece-0.1.99 sqlitedict-2.1.0 starlette-0.27.0 tabledata-1.3.3 tcolorpy-0.1.4 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.2.0 tokenizers-0.15.0 torchmetrics-1.2.1 tqdm-4.66.1 tqdm-multiprocess-0.0.11 transformers-4.35.2 typepy-1.3.2 typeshed-client-2.4.0 typing-extensions-4.9.0 tzdata-2023.3 uvicorn-0.24.0.post1 werkzeug-3.0.1 xxhash-3.4.1 yarl-1.9.4 zstandard-0.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements-all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be96f5bf-334b-4add-862f-3cbbb8623be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|                                   | 0/5 [00:00<?, ?it/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 776/776 [00:00<00:00, 8.99MB/s]\u001b[A\n",
      "\n",
      "pytorch_model.bin:   0%|                            | 0.00/4.40G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/1.84M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 19.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "generation_config.json: 100%|███████████████████| 129/129 [00:00<00:00, 820kB/s]\u001b[A\u001b[A\u001b[A\n",
      "Fetching 5 files:  20%|█████▍                     | 1/5 [00:01<00:04,  1.18s/it]\n",
      "pytorch_model.bin:   0%|                   | 10.5M/4.40G [00:00<02:20, 31.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|                   | 21.0M/4.40G [00:00<01:35, 45.7MB/s]\u001b[A\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 3.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model.bin:   1%|▏                  | 31.5M/4.40G [00:00<01:22, 52.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▏                  | 41.9M/4.40G [00:00<01:19, 55.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▏                  | 52.4M/4.40G [00:01<01:16, 56.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▎                  | 62.9M/4.40G [00:01<01:13, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▎                  | 73.4M/4.40G [00:01<01:14, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▎                  | 83.9M/4.40G [00:01<01:13, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▍                  | 94.4M/4.40G [00:01<01:11, 60.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▍                   | 105M/4.40G [00:01<01:13, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▌                   | 115M/4.40G [00:02<01:11, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▌                   | 126M/4.40G [00:02<01:12, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▌                   | 136M/4.40G [00:02<01:12, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▋                   | 147M/4.40G [00:02<01:11, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▋                   | 157M/4.40G [00:02<01:10, 60.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▊                   | 168M/4.40G [00:02<01:18, 54.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▊                   | 178M/4.40G [00:03<01:16, 55.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▊                   | 189M/4.40G [00:03<01:14, 56.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▉                   | 199M/4.40G [00:03<01:13, 57.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▉                   | 210M/4.40G [00:03<01:13, 56.8MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|█                   | 220M/4.40G [00:03<01:11, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|█                   | 231M/4.40G [00:04<01:10, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|█                   | 241M/4.40G [00:04<01:10, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▏                  | 252M/4.40G [00:04<01:12, 57.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▏                  | 262M/4.40G [00:04<01:09, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▏                  | 273M/4.40G [00:04<01:09, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▎                  | 283M/4.40G [00:04<01:09, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▎                  | 294M/4.40G [00:05<01:10, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▍                  | 304M/4.40G [00:05<01:08, 59.9MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▍                  | 315M/4.40G [00:05<01:12, 56.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▍                  | 325M/4.40G [00:05<01:07, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▌                  | 336M/4.40G [00:05<01:09, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▌                  | 346M/4.40G [00:06<01:07, 60.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▌                  | 357M/4.40G [00:06<01:07, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▋                  | 367M/4.40G [00:06<01:07, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▋                  | 377M/4.40G [00:06<01:09, 57.6MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▊                  | 388M/4.40G [00:06<01:08, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▊                  | 398M/4.40G [00:06<01:17, 51.6MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▊                  | 409M/4.40G [00:07<01:11, 56.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█▉                  | 419M/4.40G [00:07<01:02, 64.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█▉                  | 430M/4.40G [00:07<01:04, 62.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|██                  | 440M/4.40G [00:07<01:03, 62.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|██                  | 451M/4.40G [00:07<01:04, 61.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|██                  | 461M/4.40G [00:07<01:06, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▏                 | 472M/4.40G [00:08<01:09, 56.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▏                 | 482M/4.40G [00:08<01:05, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▏                 | 493M/4.40G [00:08<01:07, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▎                 | 503M/4.40G [00:08<01:05, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▎                 | 514M/4.40G [00:08<01:03, 61.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▍                 | 524M/4.40G [00:09<01:05, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▍                 | 535M/4.40G [00:09<01:04, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▍                 | 545M/4.40G [00:09<01:03, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▌                 | 556M/4.40G [00:09<01:09, 55.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▌                 | 566M/4.40G [00:09<01:08, 56.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▌                 | 577M/4.40G [00:09<01:07, 56.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▋                 | 587M/4.40G [00:10<01:07, 56.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▋                 | 598M/4.40G [00:10<01:05, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▊                 | 608M/4.40G [00:10<01:05, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▊                 | 619M/4.40G [00:10<01:06, 56.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▊                 | 629M/4.40G [00:10<01:06, 57.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|██▉                 | 640M/4.40G [00:11<01:09, 53.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|██▉                 | 650M/4.40G [00:11<01:09, 53.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|███                 | 671M/4.40G [00:11<00:59, 62.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|███                 | 682M/4.40G [00:11<00:59, 62.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▏                | 692M/4.40G [00:11<01:00, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▏                | 703M/4.40G [00:12<01:00, 60.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▏                | 713M/4.40G [00:12<01:01, 59.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▎                | 724M/4.40G [00:12<01:01, 60.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▎                | 734M/4.40G [00:12<01:03, 58.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▍                | 744M/4.40G [00:12<01:01, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▍                | 755M/4.40G [00:12<01:02, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▍                | 765M/4.40G [00:13<01:04, 56.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▌                | 776M/4.40G [00:13<01:00, 60.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▌                | 786M/4.40G [00:13<00:59, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▌                | 797M/4.40G [00:13<00:59, 60.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▋                | 807M/4.40G [00:13<01:01, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▋                | 818M/4.40G [00:14<01:01, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▊                | 828M/4.40G [00:14<01:00, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▊                | 839M/4.40G [00:14<00:59, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▊                | 849M/4.40G [00:14<01:00, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|███▉                | 860M/4.40G [00:14<00:58, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|███▉                | 870M/4.40G [00:14<00:59, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|████                | 881M/4.40G [00:15<00:58, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|████                | 891M/4.40G [00:15<01:27, 40.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▏               | 912M/4.40G [00:15<00:57, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▏               | 923M/4.40G [00:15<00:55, 62.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▏               | 933M/4.40G [00:16<00:56, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▎               | 944M/4.40G [00:16<00:57, 60.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▎               | 954M/4.40G [00:16<00:57, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▍               | 965M/4.40G [00:16<00:57, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▍               | 975M/4.40G [00:16<00:57, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▍               | 986M/4.40G [00:17<01:11, 47.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|████▎              | 1.01G/4.40G [00:17<00:54, 62.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|████▍              | 1.02G/4.40G [00:17<00:54, 61.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|████▍              | 1.03G/4.40G [00:17<00:55, 60.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▍              | 1.04G/4.40G [00:17<00:55, 60.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▌              | 1.05G/4.40G [00:17<00:55, 60.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▌              | 1.06G/4.40G [00:18<00:55, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▌              | 1.07G/4.40G [00:18<00:55, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▋              | 1.08G/4.40G [00:18<00:55, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▋              | 1.09G/4.40G [00:18<00:56, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▊              | 1.10G/4.40G [00:18<00:56, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▊              | 1.11G/4.40G [00:19<00:55, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▊              | 1.12G/4.40G [00:19<00:55, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|████▉              | 1.13G/4.40G [00:19<00:54, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|████▉              | 1.14G/4.40G [00:19<00:54, 59.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|████▉              | 1.15G/4.40G [00:19<00:54, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|█████              | 1.16G/4.40G [00:19<00:54, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████              | 1.17G/4.40G [00:20<00:54, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████              | 1.18G/4.40G [00:20<00:56, 56.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████▏             | 1.20G/4.40G [00:20<00:56, 56.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████▏             | 1.21G/4.40G [00:20<00:57, 55.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▎             | 1.22G/4.40G [00:20<00:51, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▎             | 1.23G/4.40G [00:21<00:52, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▎             | 1.24G/4.40G [00:21<00:51, 61.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▍             | 1.25G/4.40G [00:21<00:57, 55.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▍             | 1.26G/4.40G [00:21<00:59, 53.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▍             | 1.27G/4.40G [00:21<01:01, 51.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▌             | 1.28G/4.40G [00:22<01:02, 49.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▌             | 1.29G/4.40G [00:22<00:54, 57.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▌             | 1.30G/4.40G [00:22<00:50, 61.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▋             | 1.31G/4.40G [00:22<00:49, 61.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▋             | 1.32G/4.40G [00:22<00:50, 60.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▊             | 1.33G/4.40G [00:22<00:48, 63.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|█████▊             | 1.34G/4.40G [00:23<00:49, 62.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|█████▊             | 1.35G/4.40G [00:23<00:49, 61.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|█████▉             | 1.36G/4.40G [00:23<00:50, 60.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|█████▉             | 1.37G/4.40G [00:23<00:50, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|█████▉             | 1.38G/4.40G [00:23<00:50, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████             | 1.39G/4.40G [00:23<00:52, 56.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████             | 1.41G/4.40G [00:24<00:53, 55.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████             | 1.42G/4.40G [00:24<00:48, 61.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████▏            | 1.43G/4.40G [00:24<00:49, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▏            | 1.44G/4.40G [00:24<00:48, 60.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▏            | 1.45G/4.40G [00:24<00:50, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▎            | 1.46G/4.40G [00:24<00:51, 56.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▎            | 1.47G/4.40G [00:25<00:49, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▍            | 1.48G/4.40G [00:25<00:49, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▍            | 1.49G/4.40G [00:25<00:47, 60.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▍            | 1.50G/4.40G [00:25<00:48, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▌            | 1.51G/4.40G [00:25<00:48, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▌            | 1.52G/4.40G [00:26<00:48, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▌            | 1.53G/4.40G [00:26<00:48, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▋            | 1.54G/4.40G [00:26<00:48, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▋            | 1.55G/4.40G [00:26<00:48, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|██████▋            | 1.56G/4.40G [00:26<00:47, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|██████▊            | 1.57G/4.40G [00:26<00:47, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|██████▊            | 1.58G/4.40G [00:27<00:47, 59.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|██████▉            | 1.59G/4.40G [00:27<00:47, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|██████▉            | 1.60G/4.40G [00:27<00:47, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|██████▉            | 1.61G/4.40G [00:27<00:47, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███████            | 1.63G/4.40G [00:27<00:48, 57.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███████            | 1.64G/4.40G [00:28<00:57, 48.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███████            | 1.65G/4.40G [00:28<00:50, 54.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▏           | 1.66G/4.40G [00:28<00:43, 63.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▏           | 1.67G/4.40G [00:28<00:44, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▏           | 1.68G/4.40G [00:28<00:44, 60.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▎           | 1.69G/4.40G [00:28<00:45, 59.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▎           | 1.70G/4.40G [00:29<00:45, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▍           | 1.71G/4.40G [00:29<00:45, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▍           | 1.72G/4.40G [00:29<00:45, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▍           | 1.73G/4.40G [00:29<00:46, 57.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▌           | 1.74G/4.40G [00:29<00:45, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▌           | 1.75G/4.40G [00:30<00:47, 56.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▌           | 1.76G/4.40G [00:30<00:44, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▋           | 1.77G/4.40G [00:30<00:53, 49.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|███████▋           | 1.79G/4.40G [00:30<00:41, 62.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|███████▊           | 1.80G/4.40G [00:30<00:42, 61.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|███████▊           | 1.81G/4.40G [00:31<00:42, 61.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|███████▉           | 1.82G/4.40G [00:31<00:42, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|███████▉           | 1.84G/4.40G [00:31<00:56, 45.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████████           | 1.86G/4.40G [00:31<00:40, 63.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████████           | 1.87G/4.40G [00:31<00:40, 62.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████           | 1.88G/4.40G [00:32<00:40, 62.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████▏          | 1.89G/4.40G [00:32<00:40, 62.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████▏          | 1.90G/4.40G [00:32<00:43, 56.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████▏          | 1.91G/4.40G [00:32<00:41, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▎          | 1.92G/4.40G [00:32<00:43, 57.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▎          | 1.93G/4.40G [00:33<00:42, 57.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▍          | 1.94G/4.40G [00:33<00:45, 53.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▍          | 1.95G/4.40G [00:33<00:51, 47.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████████▍          | 1.96G/4.40G [00:33<00:49, 49.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████████▌          | 1.98G/4.40G [00:33<00:38, 63.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████████▌          | 1.99G/4.40G [00:34<00:35, 68.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████████▋          | 2.00G/4.40G [00:34<00:36, 66.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████████▋          | 2.01G/4.40G [00:34<00:37, 62.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████████▋          | 2.02G/4.40G [00:34<00:39, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████████▊          | 2.03G/4.40G [00:34<00:38, 61.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████████▊          | 2.04G/4.40G [00:34<00:39, 60.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████████▊          | 2.06G/4.40G [00:35<00:39, 59.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████████▉          | 2.07G/4.40G [00:35<00:39, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████████▉          | 2.08G/4.40G [00:35<00:39, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|█████████          | 2.09G/4.40G [00:35<00:39, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████          | 2.10G/4.40G [00:35<00:39, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████          | 2.11G/4.40G [00:36<00:39, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████▏         | 2.12G/4.40G [00:36<00:39, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████▏         | 2.13G/4.40G [00:36<00:39, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▏         | 2.14G/4.40G [00:36<00:38, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▎         | 2.15G/4.40G [00:36<00:38, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▎         | 2.16G/4.40G [00:36<00:38, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▎         | 2.17G/4.40G [00:37<00:37, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▍         | 2.18G/4.40G [00:37<00:37, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▍         | 2.19G/4.40G [00:37<00:37, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▌         | 2.20G/4.40G [00:37<00:37, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▌         | 2.21G/4.40G [00:37<00:37, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████████▌         | 2.22G/4.40G [00:38<00:36, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████████▋         | 2.23G/4.40G [00:38<00:36, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████████▋         | 2.24G/4.40G [00:38<00:36, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████████▋         | 2.25G/4.40G [00:38<00:37, 57.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████████▊         | 2.26G/4.40G [00:38<00:36, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████████▊         | 2.28G/4.40G [00:38<00:35, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████████▊         | 2.29G/4.40G [00:39<00:35, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████████▉         | 2.30G/4.40G [00:39<00:35, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████████▉         | 2.31G/4.40G [00:39<00:35, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████         | 2.32G/4.40G [00:39<00:35, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████         | 2.33G/4.40G [00:39<00:35, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████         | 2.34G/4.40G [00:39<00:35, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████▏        | 2.35G/4.40G [00:40<00:35, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▏        | 2.36G/4.40G [00:40<00:34, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▏        | 2.37G/4.40G [00:40<00:34, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▎        | 2.38G/4.40G [00:40<00:34, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▎        | 2.39G/4.40G [00:40<00:34, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▎        | 2.40G/4.40G [00:41<00:34, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▍        | 2.41G/4.40G [00:41<00:34, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▍        | 2.42G/4.40G [00:41<00:34, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▌        | 2.43G/4.40G [00:41<00:33, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|██████████▌        | 2.44G/4.40G [00:41<00:33, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|██████████▌        | 2.45G/4.40G [00:41<00:33, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|██████████▋        | 2.46G/4.40G [00:42<00:33, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|██████████▋        | 2.47G/4.40G [00:42<00:32, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|██████████▋        | 2.49G/4.40G [00:42<00:41, 46.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|██████████▊        | 2.51G/4.40G [00:42<00:30, 62.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|██████████▊        | 2.52G/4.40G [00:43<00:30, 61.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|██████████▉        | 2.53G/4.40G [00:43<00:30, 60.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|██████████▉        | 2.54G/4.40G [00:43<00:31, 60.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|███████████        | 2.55G/4.40G [00:43<00:30, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|███████████        | 2.56G/4.40G [00:43<00:30, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|███████████        | 2.57G/4.40G [00:43<00:30, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▏       | 2.58G/4.40G [00:44<00:31, 57.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▏       | 2.59G/4.40G [00:44<00:31, 57.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▏       | 2.60G/4.40G [00:44<00:31, 57.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▎       | 2.61G/4.40G [00:44<00:29, 60.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▎       | 2.62G/4.40G [00:44<00:31, 57.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▎       | 2.63G/4.40G [00:45<00:30, 57.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▍       | 2.64G/4.40G [00:45<00:30, 57.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▍       | 2.65G/4.40G [00:45<00:29, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|███████████▌       | 2.66G/4.40G [00:45<00:28, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|███████████▌       | 2.67G/4.40G [00:45<00:28, 60.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|███████████▌       | 2.68G/4.40G [00:45<00:28, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|███████████▋       | 2.69G/4.40G [00:46<00:28, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|███████████▋       | 2.71G/4.40G [00:46<00:28, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|███████████▋       | 2.72G/4.40G [00:46<00:28, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|███████████▊       | 2.73G/4.40G [00:46<00:28, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|███████████▊       | 2.74G/4.40G [00:46<00:28, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|███████████▊       | 2.75G/4.40G [00:46<00:28, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|███████████▉       | 2.76G/4.40G [00:47<00:28, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|███████████▉       | 2.77G/4.40G [00:47<00:28, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|███████████▉       | 2.78G/4.40G [00:47<00:27, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|████████████       | 2.79G/4.40G [00:47<00:27, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████       | 2.80G/4.40G [00:47<00:27, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▏      | 2.81G/4.40G [00:48<00:27, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▏      | 2.82G/4.40G [00:48<00:26, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▏      | 2.83G/4.40G [00:48<00:26, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▎      | 2.84G/4.40G [00:48<00:26, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▎      | 2.85G/4.40G [00:48<00:26, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▎      | 2.86G/4.40G [00:48<00:26, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▍      | 2.87G/4.40G [00:49<00:26, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|████████████▍      | 2.88G/4.40G [00:49<00:26, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|████████████▍      | 2.89G/4.40G [00:49<00:25, 58.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|████████████▌      | 2.90G/4.40G [00:49<00:25, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|████████████▌      | 2.92G/4.40G [00:49<00:25, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|████████████▋      | 2.93G/4.40G [00:49<00:25, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|████████████▋      | 2.94G/4.40G [00:50<00:24, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|████████████▋      | 2.95G/4.40G [00:50<00:24, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|████████████▊      | 2.96G/4.40G [00:50<00:24, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|████████████▊      | 2.97G/4.40G [00:50<00:24, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|████████████▊      | 2.98G/4.40G [00:50<00:24, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|████████████▉      | 2.99G/4.40G [00:51<00:24, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|████████████▉      | 3.00G/4.40G [00:51<00:24, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|████████████▉      | 3.01G/4.40G [00:51<00:23, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████      | 3.02G/4.40G [00:51<00:23, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████      | 3.03G/4.40G [00:51<00:23, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████▏     | 3.04G/4.40G [00:51<00:23, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████▏     | 3.05G/4.40G [00:52<00:22, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▏     | 3.06G/4.40G [00:52<00:22, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▎     | 3.07G/4.40G [00:52<00:22, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▎     | 3.08G/4.40G [00:52<00:22, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▎     | 3.09G/4.40G [00:52<00:22, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▍     | 3.10G/4.40G [00:53<00:22, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▍     | 3.11G/4.40G [00:53<00:22, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▍     | 3.12G/4.40G [00:53<00:21, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▌     | 3.14G/4.40G [00:53<00:21, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▌     | 3.15G/4.40G [00:53<00:21, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▋     | 3.16G/4.40G [00:53<00:21, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▋     | 3.17G/4.40G [00:54<00:21, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▋     | 3.18G/4.40G [00:54<00:20, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▊     | 3.19G/4.40G [00:54<00:20, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|█████████████▊     | 3.20G/4.40G [00:54<00:20, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|█████████████▊     | 3.21G/4.40G [00:55<00:26, 45.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|█████████████▉     | 3.23G/4.40G [00:55<00:18, 62.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|█████████████▉     | 3.24G/4.40G [00:55<00:18, 61.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|██████████████     | 3.25G/4.40G [00:55<00:19, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|██████████████     | 3.26G/4.40G [00:55<00:19, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|██████████████▏    | 3.27G/4.40G [00:55<00:19, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▏    | 3.28G/4.40G [00:56<00:19, 57.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▏    | 3.29G/4.40G [00:56<00:19, 58.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▎    | 3.30G/4.40G [00:56<00:17, 61.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▎    | 3.31G/4.40G [00:56<00:17, 61.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▎    | 3.32G/4.40G [00:56<00:17, 60.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▍    | 3.33G/4.40G [00:56<00:17, 59.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▍    | 3.34G/4.40G [00:57<00:17, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▍    | 3.36G/4.40G [00:57<00:17, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▌    | 3.37G/4.40G [00:57<00:17, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▌    | 3.38G/4.40G [00:57<00:17, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▌    | 3.39G/4.40G [00:57<00:17, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▋    | 3.40G/4.40G [00:58<00:17, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▋    | 3.41G/4.40G [00:58<00:16, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▊    | 3.42G/4.40G [00:58<00:16, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▊    | 3.43G/4.40G [00:58<00:16, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▊    | 3.44G/4.40G [00:58<00:16, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▉    | 3.45G/4.40G [00:58<00:16, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|██████████████▉    | 3.46G/4.40G [00:59<00:16, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|██████████████▉    | 3.47G/4.40G [00:59<00:16, 57.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████████████    | 3.48G/4.40G [00:59<00:15, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████████████    | 3.49G/4.40G [00:59<00:15, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████    | 3.50G/4.40G [00:59<00:15, 58.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████▏   | 3.51G/4.40G [01:00<00:14, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████▏   | 3.52G/4.40G [01:00<00:14, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████▎   | 3.53G/4.40G [01:00<00:14, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▎   | 3.54G/4.40G [01:00<00:14, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▎   | 3.55G/4.40G [01:00<00:14, 57.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▍   | 3.57G/4.40G [01:00<00:14, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▍   | 3.58G/4.40G [01:01<00:14, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▍   | 3.59G/4.40G [01:01<00:14, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▌   | 3.60G/4.40G [01:01<00:13, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▌   | 3.61G/4.40G [01:01<00:13, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▌   | 3.62G/4.40G [01:01<00:13, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▋   | 3.63G/4.40G [01:01<00:13, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▋   | 3.64G/4.40G [01:02<00:13, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▊   | 3.65G/4.40G [01:02<00:12, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▊   | 3.66G/4.40G [01:02<00:12, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▊   | 3.67G/4.40G [01:02<00:12, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|███████████████▉   | 3.68G/4.40G [01:02<00:12, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|███████████████▉   | 3.69G/4.40G [01:03<00:12, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|███████████████▉   | 3.70G/4.40G [01:03<00:11, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|████████████████   | 3.71G/4.40G [01:03<00:11, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████   | 3.72G/4.40G [01:03<00:11, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████   | 3.73G/4.40G [01:03<00:11, 57.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████▏  | 3.74G/4.40G [01:03<00:11, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████▏  | 3.75G/4.40G [01:04<00:10, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▎  | 3.76G/4.40G [01:04<00:10, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▎  | 3.77G/4.40G [01:04<00:10, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▎  | 3.79G/4.40G [01:04<00:10, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▍  | 3.80G/4.40G [01:04<00:10, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▍  | 3.81G/4.40G [01:05<00:10, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▍  | 3.82G/4.40G [01:05<00:12, 47.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▌  | 3.84G/4.40G [01:05<00:09, 61.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▌  | 3.85G/4.40G [01:05<00:09, 61.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▋  | 3.86G/4.40G [01:05<00:09, 60.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▋  | 3.87G/4.40G [01:06<00:08, 59.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▊  | 3.88G/4.40G [01:06<00:08, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▊  | 3.89G/4.40G [01:06<00:08, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▊  | 3.90G/4.40G [01:06<00:08, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▉  | 3.91G/4.40G [01:06<00:09, 53.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▉  | 3.92G/4.40G [01:07<00:09, 49.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▉  | 3.93G/4.40G [01:07<00:08, 53.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████  | 3.94G/4.40G [01:07<00:07, 57.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████  | 3.95G/4.40G [01:07<00:07, 61.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████  | 3.96G/4.40G [01:07<00:07, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████▏ | 3.97G/4.40G [01:07<00:06, 63.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▏ | 3.98G/4.40G [01:08<00:07, 57.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 4.00G/4.40G [01:08<00:06, 62.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 4.01G/4.40G [01:08<00:06, 62.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 4.02G/4.40G [01:08<00:06, 61.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▍ | 4.03G/4.40G [01:08<00:06, 60.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▍ | 4.04G/4.40G [01:08<00:06, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▍ | 4.05G/4.40G [01:09<00:05, 60.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▌ | 4.06G/4.40G [01:09<00:05, 59.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▌ | 4.07G/4.40G [01:09<00:05, 59.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▌ | 4.08G/4.40G [01:09<00:05, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 4.09G/4.40G [01:09<00:05, 59.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 4.10G/4.40G [01:10<00:05, 59.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 4.11G/4.40G [01:10<00:04, 58.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▊ | 4.12G/4.40G [01:10<00:04, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▊ | 4.13G/4.40G [01:10<00:04, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▉ | 4.14G/4.40G [01:10<00:04, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▉ | 4.15G/4.40G [01:10<00:04, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|█████████████████▉ | 4.16G/4.40G [01:11<00:04, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|██████████████████ | 4.17G/4.40G [01:11<00:03, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|██████████████████ | 4.18G/4.40G [01:11<00:03, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|██████████████████ | 4.19G/4.40G [01:11<00:03, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▏| 4.20G/4.40G [01:11<00:03, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▏| 4.22G/4.40G [01:12<00:03, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▏| 4.23G/4.40G [01:12<00:02, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▎| 4.24G/4.40G [01:12<00:02, 58.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▎| 4.25G/4.40G [01:12<00:02, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▍| 4.26G/4.40G [01:12<00:02, 58.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▍| 4.27G/4.40G [01:12<00:02, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▍| 4.28G/4.40G [01:13<00:02, 59.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▌| 4.29G/4.40G [01:13<00:01, 59.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▌| 4.30G/4.40G [01:13<00:01, 58.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▌| 4.31G/4.40G [01:13<00:01, 58.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▋| 4.32G/4.40G [01:13<00:01, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▋| 4.33G/4.40G [01:13<00:01, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▋| 4.34G/4.40G [01:14<00:01, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▊| 4.35G/4.40G [01:14<00:00, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▊| 4.36G/4.40G [01:14<00:00, 58.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▉| 4.37G/4.40G [01:14<00:00, 58.2MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|██████████████████▉| 4.38G/4.40G [01:14<00:00, 58.4MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|██████████████████▉| 4.39G/4.40G [01:15<00:00, 58.6MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████| 4.40G/4.40G [01:15<00:00, 58.5MB/s]\u001b[A\n",
      "Fetching 5 files: 100%|███████████████████████████| 5/5 [01:26<00:00, 17.38s/it]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/download.py --repo_id TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d86c23-9c12-42b8-933b-c3d5c2fdb21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Model config {'name': 'tiny-llama-1.1b', 'hf_config': {'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-955k-token-2T'}, 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 22, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 4, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 5632, 'rope_condense_ratio': 1, 'rope_base': 10000}\n",
      "Processing checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T/pytorch_model.bin\n",
      "Loading 'lm_head.weight' into RAM\n",
      "Loading 'model.embed_tokens.weight' into RAM\n",
      "Loading 'model.layers.0.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.0.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.0.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.1.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.1.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.1.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.2.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.2.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.2.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.3.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.3.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.3.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.4.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.4.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.4.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.5.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.5.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.5.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.6.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.6.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.6.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.7.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.7.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.7.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.8.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.8.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.8.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.9.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.9.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.9.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.10.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.10.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.10.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.11.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.11.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.11.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.12.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.12.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.12.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.13.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.13.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.13.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.14.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.14.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.14.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.15.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.15.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.15.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.16.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.16.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.16.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.17.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.17.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.17.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.18.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.18.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.18.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.18.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.18.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.18.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.19.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.19.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.19.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.19.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.19.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.19.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.20.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.20.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.20.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.20.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.20.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.20.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.21.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.21.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.21.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.21.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.21.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.21.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.norm.weight' into RAM\n",
      "Loading 'layer 0 q' into RAM\n",
      "Loading 'layer 0 k' into RAM\n",
      "Loading 'layer 0 v' into RAM\n",
      "Loading 'layer 1 q' into RAM\n",
      "Loading 'layer 1 k' into RAM\n",
      "Loading 'layer 1 v' into RAM\n",
      "Loading 'layer 2 q' into RAM\n",
      "Loading 'layer 2 k' into RAM\n",
      "Loading 'layer 2 v' into RAM\n",
      "Loading 'layer 3 q' into RAM\n",
      "Loading 'layer 3 k' into RAM\n",
      "Loading 'layer 3 v' into RAM\n",
      "Loading 'layer 4 q' into RAM\n",
      "Loading 'layer 4 k' into RAM\n",
      "Loading 'layer 4 v' into RAM\n",
      "Loading 'layer 5 q' into RAM\n",
      "Loading 'layer 5 k' into RAM\n",
      "Loading 'layer 5 v' into RAM\n",
      "Loading 'layer 6 q' into RAM\n",
      "Loading 'layer 6 k' into RAM\n",
      "Loading 'layer 6 v' into RAM\n",
      "Loading 'layer 7 q' into RAM\n",
      "Loading 'layer 7 k' into RAM\n",
      "Loading 'layer 7 v' into RAM\n",
      "Loading 'layer 8 q' into RAM\n",
      "Loading 'layer 8 k' into RAM\n",
      "Loading 'layer 8 v' into RAM\n",
      "Loading 'layer 9 q' into RAM\n",
      "Loading 'layer 9 k' into RAM\n",
      "Loading 'layer 9 v' into RAM\n",
      "Loading 'layer 10 q' into RAM\n",
      "Loading 'layer 10 k' into RAM\n",
      "Loading 'layer 10 v' into RAM\n",
      "Loading 'layer 11 q' into RAM\n",
      "Loading 'layer 11 k' into RAM\n",
      "Loading 'layer 11 v' into RAM\n",
      "Loading 'layer 12 q' into RAM\n",
      "Loading 'layer 12 k' into RAM\n",
      "Loading 'layer 12 v' into RAM\n",
      "Loading 'layer 13 q' into RAM\n",
      "Loading 'layer 13 k' into RAM\n",
      "Loading 'layer 13 v' into RAM\n",
      "Loading 'layer 14 q' into RAM\n",
      "Loading 'layer 14 k' into RAM\n",
      "Loading 'layer 14 v' into RAM\n",
      "Loading 'layer 15 q' into RAM\n",
      "Loading 'layer 15 k' into RAM\n",
      "Loading 'layer 15 v' into RAM\n",
      "Loading 'layer 16 q' into RAM\n",
      "Loading 'layer 16 k' into RAM\n",
      "Loading 'layer 16 v' into RAM\n",
      "Loading 'layer 17 q' into RAM\n",
      "Loading 'layer 17 k' into RAM\n",
      "Loading 'layer 17 v' into RAM\n",
      "Loading 'layer 18 q' into RAM\n",
      "Loading 'layer 18 k' into RAM\n",
      "Loading 'layer 18 v' into RAM\n",
      "Loading 'layer 19 q' into RAM\n",
      "Loading 'layer 19 k' into RAM\n",
      "Loading 'layer 19 v' into RAM\n",
      "Loading 'layer 20 q' into RAM\n",
      "Loading 'layer 20 k' into RAM\n",
      "Loading 'layer 20 v' into RAM\n",
      "Loading 'layer 21 q' into RAM\n",
      "Loading 'layer 21 k' into RAM\n",
      "Loading 'layer 21 v' into RAM\n",
      "Saving converted checkpoint\n"
     ]
    }
   ],
   "source": [
    "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6288e0ac-18fc-4e39-9723-ba088cbd1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file...\n",
      "Loading tokenizer...\n",
      "train has 2,643 samples\n",
      "test has 106 samples\n",
      "Processing train split ...\n",
      "100%|██████████████████████████████████████| 2643/2643 [00:02<00:00, 905.91it/s]\n",
      "Processing test split ...\n",
      "100%|████████████████████████████████████████| 106/106 [00:00<00:00, 900.10it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84fc3a6e-dc64-442e-91d6-612a53b975f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bdc589f-7ebc-464d-912d-4bb1fe31c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 4, 'gradient_accumulation_iters': 32, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model 'checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T/lit_model.pth' with {'name': 'tiny-llama-1.1b', 'hf_config': {'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-955k-token-2T'}, 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 22, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 4, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 5632, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 64, 'rope_n_elem': 64}\n",
      "Number of trainable parameters: 1,126,400\n",
      "Number of non trainable parameters: 1,100,048,384\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 1388, the model's maximum sequence length is 1388 and context length is 2048\n",
      "Validating ...\n",
      "I don't know how else to explain it. All I can say is that I feel empty, I feel nothing. How do I stop feeling this way?\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "I don't know how else to explain it. All I can say is that I feel empty, I feel nothing. How do I stop feeling this way?\n",
      "\n",
      "### Response:\n",
      "I don't know how else to explain it. All I can say is that I feel empty, I feel nothing. How do I stop feeling this way?\n",
      "\n",
      "## Comment\n",
      " <reponame>microshm/vcpkg\n",
      "# 10.10.0\n",
      "\n",
      "## Updates\n",
      "\n",
      "- [#91170](https://github.com/Microsoft/vcpkg/pull/91170) -\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/lit-gpt/finetune/lora.py\", line 318, in <module>\n",
      "    CLI(setup)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 96, in CLI\n",
      "    return _run_component(components, cfg_init)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 181, in _run_component\n",
      "    return component(**cfg)\n",
      "  File \"/workspace/lit-gpt/finetune/lora.py\", line 96, in setup\n",
      "    fabric.launch(main, data_dir, checkpoint_dir, out_dir)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/workspace/lit-gpt/finetune/lora.py\", line 151, in main\n",
      "    train(fabric, model, optimizer, scheduler, train_data, val_data, checkpoint_dir, out_dir)\n",
      "  File \"/workspace/lit-gpt/finetune/lora.py\", line 203, in train\n",
      "    fabric.backward(loss / gradient_accumulation_iters)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 422, in backward\n",
      "    self._strategy.backward(tensor, module, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\", line 192, in backward\n",
      "    self.precision.backward(tensor, module, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/precision.py\", line 107, in backward\n",
      "    tensor.backward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 17.12 MiB is free. Process 1890861 has 15.71 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 457.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py --checkpoint_dir checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T --precision bf16-true --quantize bnb.nf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c6b4f-aa73-43c2-9268-f98c3c7a5bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
